{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# Understanding Hired Rides in NYC\n",
    "\n",
    "_[Project prompt](https://docs.google.com/document/d/1VERPjEZcC1XSs4-02aM-DbkNr_yaJVbFjLJxaYQswqA/edit#)_\n",
    "\n",
    "_This scaffolding notebook may be used to help setup your final project. It's **totally optional** whether you make use of this or not._\n",
    "\n",
    "_If you do use this notebook, everything provided is optional as well - you may remove or add prose and code as you wish._\n",
    "\n",
    "_Anything in italics (prose) or comments (in code) is meant to provide you with guidance. **Remove the italic lines and provided comments** before submitting the project, if you choose to use this scaffolding. We don't need the guidance when grading._\n",
    "\n",
    "_**All code below should be consider \"pseudo-code\" - not functional by itself, and only a suggestion at the approach.**_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25627e8d",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project._\n",
    "\n",
    "* Code clarity: make sure the code conforms to:\n",
    "    * [ ] [PEP 8](https://peps.python.org/pep-0008/) - You might find [this resource](https://realpython.com/python-pep8/) helpful as well as [this](https://github.com/dnanhkhoa/nb_black) or [this](https://jupyterlab-code-formatter.readthedocs.io/en/latest/) tool\n",
    "    * [ ] [PEP 257](https://peps.python.org/pep-0257/)\n",
    "    * [ ] Break each task down into logical functions\n",
    "* The following files are submitted for the project (see the project's GDoc for more details):\n",
    "    * [ ] `README.md`\n",
    "    * [ ] `requirements.txt`\n",
    "    * [ ] `.gitignore`\n",
    "    * [ ] `schema.sql`\n",
    "    * [ ] 6 query files (using the `.sql` extension), appropriately named for the purpose of the query\n",
    "    * [x] Jupyter Notebook containing the project (this file!)\n",
    "* [x] You can edit this cell and add a `x` inside the `[ ]` like this task to denote a completed task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project, for example:\n",
    "\n",
    "\n",
    "\n",
    "import math\n",
    "import geopandas as gpd\n",
    "from math import sin, cos, asin, pi, sqrt\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants we need for processing data:\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "UBER_CSV = \"uber_rides_sample.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf38168",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Define a function that calculates the distance between two coordinates in kilometers that **only uses the `math` module** from the standard library.\n",
    "* [ ] Taxi data:\n",
    "    * [ ] Use the `re` module, and the packages `requests`, BeautifulSoup (`bs4`), and (optionally) `pandas` to programmatically download the required CSV files & load into memory.\n",
    "    * You may need to do this one file at a time - download, clean, sample. You can cache the sampling by saving it as a CSV file (and thereby freeing up memory on your computer) before moving onto the next file. \n",
    "* [ ] Weather & Uber data:\n",
    "    * [ ] Download the data manually in the link provided in the project doc.\n",
    "* [ ] All data:\n",
    "    * [ ] Load the data using `pandas`\n",
    "    * [ ] Clean the data, including:\n",
    "        * Remove unnecessary columns\n",
    "        * Remove invalid data points (take a moment to consider what's invalid)\n",
    "        * Normalize column names\n",
    "        * (Taxi & Uber data) Remove trips that start and/or end outside the designated [coordinate box](http://bboxfinder.com/#40.560445,-74.242330,40.908524,-73.717047)\n",
    "    * [ ] (Taxi data) Sample the data so that you have roughly the same amount of data points over the given date range for both Taxi data and Uber data.\n",
    "* [ ] Weather data:\n",
    "    * [ ] Split into two `pandas` DataFrames: one for required hourly data, and one for the required daily daya.\n",
    "    * [ ] You may find that the weather data you need later on does not exist at the frequency needed (daily vs hourly). You may calculate/generate samples from one to populate the other. Just document what youâ€™re doing so we can follow along. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculating distance\n",
    "_Using the latituede/longitude given in the Yellow Taxi Dataframe to calculate the route distance._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance(from_coord, to_coord):\n",
    "    \"\"\"\n",
    "    Calculate the distance between two coordinates\n",
    "    \n",
    "    The function takes in 2 argumants: \"from_coord\" and \"to_coord\"\n",
    "    \"from_coord\": columns 'pickup_latitude' and 'pickup_longitude' from datarame\n",
    "    \"to_coord\":  columns 'dropoff_latitude' and 'dropoff_longitude' from datarame\n",
    "    \n",
    "    RETURN: an array of calculated distances (in km)\n",
    "    \"\"\"\n",
    "    # first change the latitude and longitude into radians\n",
    "    lat1 = from_coord['pickup_latitude'] *pi/180\n",
    "    lon1 = from_coord['pickup_longitude'] *pi/180\n",
    "    lat2 = to_coord['dropoff_latitude']*pi/180\n",
    "    lon2 = to_coord['dropoff_longitude']*pi/180\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    \n",
    "    # using Haversine formula to calculate distance\n",
    "    a = ((dlat/2).map(sin))**2 + (lat1.map(cos)) * (lat2.map(cos)) * ((dlon/2).map(sin))**2\n",
    "    \n",
    "    # suppose the radius of earth is 6371 km\n",
    "    distance = 6371*2 * (a.map(sqrt)).map(asin)\n",
    "    \n",
    "    # round the distance with 2 decimals\n",
    "    return round(distance, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d6abf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):\n",
    "    \"\"\"\n",
    "    Add one column 'distance' which calculates taxi route distance into 'dataframe'\n",
    "    \n",
    "    The function takes one argument: 'dataframe'\n",
    "    Use columns \"pickup_latitude\" and \"pickup_longitude\" to create argument \"from_coord\"\n",
    "    Use columns \"dropoff_latitude\" and \"dropoff_longitude\" to create argument to_coord\"\n",
    "    \n",
    "    RETURN: dataframe with new column 'distance' \n",
    "    \"\"\"\n",
    "    # generate 'from_coord' & 'to_coord'\n",
    "    from_coord = dataframe[['pickup_latitude', 'pickup_longitude']]\n",
    "    to_coord = dataframe[['dropoff_latitude', 'dropoff_longitude']]\n",
    "    \n",
    "    # add 'distance' into dataframe\n",
    "    dataframe['distance'] = calculate_distance(from_coord, to_coord)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Processing Taxi Data\n",
    "\n",
    "_Obtain Yellow Taxi Data from 2009-01 to 2015-06, then clean and combine all dataframes._\n",
    "\n",
    "_Firstly we utilize `re`, `requests` and `BeautifulSoup` module to find specific data._\n",
    "\n",
    "_Then we use `pandas` to load and clean the monthly data according to our rules._\n",
    "\n",
    "_Finally we combine each monthly data into a giant one._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_taxi_csv_urls():\n",
    "    \"\"\"\n",
    "    Get all the URLs of Taxi Data\n",
    "    RETURN: a lits of links that download parquet files of Taxi Data\n",
    "    \"\"\"\n",
    "    y = []\n",
    "    links = []\n",
    "    # get response from HTML\n",
    "    response = requests.get(TAXI_URL)\n",
    "    html = response.content\n",
    "    \n",
    "    # use bs4 to parse html and find Yellow Taxi urls\n",
    "    soup = bs4.BeautifulSoup(html, \"html.parser\")\n",
    "    yellow = soup.find_all(\"a\", attrs = {\"title\":\"Yellow Taxi Trip Records\"})\n",
    "    y.append([a[\"href\"] for a in yellow])\n",
    "    \n",
    "    # use regex pattern to find desired URLs from 'yellow taxi trip records'\n",
    "    pattern = re.compile(\".*(2009|201[0-4]|2015-0[1-6]).*\")\n",
    "    for i in y[0]:\n",
    "        match = re.match(pattern, i)\n",
    "        if match != None:\n",
    "            links.append(i)\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month_taxi_data(url):\n",
    "    \"\"\"\n",
    "    The function takes in one url from Internet\n",
    "    Obtain dataframe from url of every given month and clean the df\n",
    "    \n",
    "    RETURN: a cleaned dataframe\n",
    "    \"\"\"   \n",
    "    # Load shp file of Taxi Zones\n",
    "    g = gpd.read_file(filename='taxi_zones.zip', engine='fiona')\n",
    "    g = g.to_crs(4326)\n",
    "\n",
    "    # use the center of the zones as a point to look up the coordinates.\n",
    "    g['longitude'] = g.centroid.x\n",
    "    g['latitude'] = g.centroid.y\n",
    "\n",
    "    # remove unnecessary columns \n",
    "    # create mapping methods for latitude & longitude\n",
    "    # aim to create lat & longitude which do not exist in some taxi data\n",
    "    g = g[['LocationID', 'longitude', 'latitude', 'zone', 'borough']]\n",
    "    lat_map = dict(zip(g['LocationID'], g['latitude']))\n",
    "    lon_map = dict(zip(g['LocationID'], g['longitude']))\n",
    "    \n",
    "    df = pd.read_parquet(url)\n",
    "    \n",
    "    # process dataframe which only has LocationIDs \n",
    "    # match the LocationID in 'df' with dataframe 'g'\n",
    "    # add 4 new columns to 'df' after matching\n",
    "    if 'DOLocationID' in df:\n",
    "        df['pickup_latitude']  = df['PULocationID'].map(lat_map)\n",
    "        df['pickup_longitude'] = df['PULocationID'].map(lon_map)\n",
    "        df['dropoff_latitude'] = df['DOLocationID'].map(lat_map)\n",
    "        df['dropoff_longitude']= df['DOLocationID'].map(lon_map)\n",
    "\n",
    "    # normalizing column names, according to UBER_DATA column names\n",
    "    df = df.rename(columns={'tpep_pickup_datetime':'pickup_datetime', \n",
    "                            'Trip_Pickup_DateTime':'pickup_datetime', \n",
    "                            'Tip_Amt':'tip_amount',\n",
    "                            'Fare_Amt':'fare_amount', 'Passenger_Count':'passenger_count',\n",
    "                            'Start_Lon':'pickup_longitude','Start_Lat':'pickup_latitude', \n",
    "                            'End_Lon':'dropoff_longitude', 'End_Lat':'dropoff_latitude',           \n",
    "                            })\n",
    "        \n",
    "    # remove invalid ponits, fare, distance and passenger must be positive\n",
    "    df = df[df['fare_amount'] > 0]\n",
    "    df = df[(df['tip_amount'] >= 0)]\n",
    "    df = df[df['passenger_count'] > 0]\n",
    "    df = df[df['pickup_latitude']!=df['dropoff_latitude']]\n",
    "    df = df[df['pickup_longitude']!=df['dropoff_longitude']]\n",
    "    df = df[abs(df['pickup_longitude']-df['dropoff_longitude'])>0.001]\n",
    "        \n",
    "    # removing trips that start and/or end outside of the BOX\n",
    "    df = df[df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]]\n",
    "    df = df[df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0]]\n",
    "    df = df[df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]]\n",
    "    df = df[df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1]]\n",
    "        \n",
    "    df = df[df['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]]\n",
    "    df = df[df['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0]]\n",
    "    df = df[df['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]]\n",
    "    df = df[df['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1]]\n",
    "    \n",
    "    # remove unnecessary columns, only keep 6 columns we need\n",
    "    df = df[['pickup_datetime','tip_amount',\n",
    "             'pickup_latitude','pickup_longitude',\n",
    "             'dropoff_latitude','dropoff_longitude',\n",
    "            ]]\n",
    "    # sampling from data to match the amount of \"taxi_data\" with \"uber_data\" \n",
    "    df = df.sample(n=round(185444/78))\n",
    "    \n",
    "    # using appropriate column type\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df.iloc[:,1:6] = df.iloc[:,1:6].astype('float32')     \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data():\n",
    "    \"\"\"\n",
    "    The function generate a list of Yellow Taxi urls\n",
    "    For each url, obtain the dataframe from url\n",
    "    Then clean dataframe and add 'distance' column\n",
    "    In the end we join each month's dataframe to a giant one\n",
    "    \n",
    "    RETURN: a huge dataframe contains cleaned data from every month\n",
    "    \n",
    "    \"\"\"\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    # acquire list of Yellow Taxi urls\n",
    "    all_parquet = find_taxi_csv_urls()\n",
    "    \n",
    "    # clean dataframe and add columns\n",
    "    for i in all_parquet:\n",
    "        \n",
    "        # read local files by extracting file name from url\n",
    "        file = i.split('/')[-1]\n",
    "        dataframe = get_and_clean_month_taxi_data(file)\n",
    "        add_distance_column(dataframe)  \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        print(f'done with {file}')\n",
    "    \n",
    "    \n",
    "    # combine to a giant dataframe\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    \n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data\n",
    "\n",
    "_Processing the Uber Data, make the column type and column names the same pattern as Taxi Data._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    \"\"\"\n",
    "    The funcion takes one argument, the csv file\n",
    "    We do the same procedure as cleaning Taxi Data\n",
    "    We aim to obtain dataframe in the same colmn type as Taxi Data\n",
    "    \n",
    "    RETURN: a cleaned dataframe\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # removing trips that start and/or end outside of the BOX\n",
    "    df = df[df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]]\n",
    "    df = df[df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0]]\n",
    "    df = df[df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]]\n",
    "    df = df[df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1]]\n",
    "        \n",
    "    df = df[df['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]]\n",
    "    df = df[df['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0]]\n",
    "    df = df[df['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]]\n",
    "    df = df[df['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1]]\n",
    "    \n",
    "\n",
    "    # remove invalid ponits\n",
    "    df = df[df['fare_amount']>0]\n",
    "    df = df[df['passenger_count'] > 0]\n",
    "    df = df[df['pickup_latitude']!=df['dropoff_latitude']]\n",
    "    df = df[df['pickup_longitude']!=df['dropoff_longitude']]\n",
    "    df = df[abs(df['pickup_longitude']-df['dropoff_longitude'])>0.001]\n",
    "    \n",
    "    # remove unnecessary columns\n",
    "    df = df[['pickup_datetime',\n",
    "             'pickup_latitude','pickup_longitude',\n",
    "             'dropoff_latitude','dropoff_longitude',\n",
    "            ]]\n",
    "        \n",
    "    # using appropriate column type\n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'])\n",
    "    df.iloc[:,1:5] = df.iloc[:,1:5].astype('float32')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    \"\"\"\n",
    "    The function loads and cleans Uber_data\n",
    "    Calculate and Add 'distance' column to uber dataframe\n",
    "    \n",
    "    RETURN: cleaned dataframe with new column 'distance'\n",
    "    \"\"\"\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_CSV)\n",
    "    add_distance_column(uber_dataframe)\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data\n",
    "\n",
    "_Fecth Weather Data from 2009 to 2015 to clean._\n",
    "\n",
    "_Use `pandas` module to clean and fill missing values_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    \"\"\"\n",
    "    Takes in one argument: the hourly csv data\n",
    "    We clean the dataframe by dropping NaN value in WindSpeed\n",
    "    and filling NaN as 0 in precipitation column\n",
    "    Also change the DATE to pd.datetime type\n",
    "    \n",
    "    RETURN: a cleaned dataframe\n",
    "    \"\"\"\n",
    "    # read the csv file \n",
    "    df=pd.read_csv(csv_file)\n",
    "    \n",
    "    # clean the data\n",
    "    \n",
    "    # remove all columns that contain the world 'Daily'\n",
    "    df=df.drop(df.filter(regex='Daily').columns, axis=1)\n",
    "    \n",
    "    # remove all columns that have all NaN values\n",
    "    df=df.dropna(axis=1, how=\"all\")\n",
    "    \n",
    "    # remove all columns with only zero values\n",
    "    df.loc[:, (df != 0).any(axis=0)]\n",
    "    \n",
    "    # fill missing NaN value and drop if failed to fill\n",
    "    if df['HourlyWindSpeed'] is 'NaN':\n",
    "        df['HourlyWindSpeed'] = df['DailyAverageWindSpeed']/24\n",
    "        df['HourlyWindSpeed'].apply(lambda x: round(x,2))\n",
    "\n",
    "    # remove unnecessary columns\n",
    "    df = df[['DATE', 'HourlyWindSpeed', 'HourlyPrecipitation']]\n",
    "    df = df[df['HourlyPrecipitation']!= 'M']\n",
    "\n",
    "    # treat trace amounnt precipitation as 0; change NaN to 0\n",
    "    # change those numbers with 's' into float\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace({pd.np.nan: 0, 'T': 0})\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].apply(\n",
    "                                                                lambda x: x.replace('s', '') \n",
    "                                                                if isinstance(x, str) else x\n",
    "                                                                )\n",
    "    # use appropriate column type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].astype(float)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    \"\"\"\n",
    "    Takes in one argument: the daily csv data\n",
    "    Calculate the mean WindSpeed & Precipitation within each day and insert\n",
    "    Also change the DATE to pd.datetime type\n",
    "    \n",
    "    RETURN: a cleaned daily dataframe\n",
    "    \"\"\"\n",
    "    #read the csv file \n",
    "    df=pd.read_csv(csv_file)\n",
    "    \n",
    "    # use appropriate column type\n",
    "    df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "    \n",
    "    #remove all columns that have all NaN values\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    df = df[df['HourlyPrecipitation']!= 'M']\n",
    "\n",
    "    # treat trace amounnt precipitation as 0; change NaN to 0\n",
    "    # change those numbers with 's' into float\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace({pd.np.nan: 0, 'T': 0})\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].apply(\n",
    "                                                                lambda x: x.replace('s', '') \n",
    "                                                                if isinstance(x, str) else x\n",
    "                                                                )\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].astype(float)\n",
    "    \n",
    "    #remove all columns with only zero values\n",
    "    df.loc[:, (df != 0).any(axis=0)]\n",
    "    \n",
    "    # extract part of the date, without hour \n",
    "    df['DATE'] = df['DATE'].apply(lambda x: x.date())\n",
    "\n",
    "    # Calculate the mean of the Hourly WindSpeed & Precipitation \n",
    "    DailyWind = df.groupby('DATE')['HourlyWindSpeed'].mean()\n",
    "    DailyPreci = df.groupby('DATE')['HourlyPrecipitation'].mean()\n",
    "    \n",
    "    #combine duplicated DATE and merge the mean into new dataframe\n",
    "    df1 = df.drop_duplicates(subset=['DATE'])\n",
    "    df1 = df1[['DATE']]\n",
    "    df1 = pd.merge(df1, DailyWind, on='DATE')\n",
    "    df1 = pd.merge(df1, DailyPreci, on='DATE')\n",
    "    \n",
    "    # normalizing column name\n",
    "    df1.rename(columns={'HourlyWindSpeed':'DailyWindSpeed',\n",
    "                        'HourlyPrecipitation':'DailyPrecipitation'},inplace=True\n",
    "              )\n",
    "    \n",
    "    # round to decimals 2\n",
    "    df1['DailyWindSpeed'] = df1['DailyWindSpeed'].round(2)\n",
    "    df1['DailyPrecipitation'] = df1['DailyPrecipitation'].round(2)\n",
    "    df1['DATE'] = pd.to_datetime(df1['DATE'])\n",
    "    \n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    # add some way to find all weather CSV files\n",
    "    # or just add the name/paths manually\n",
    "    \n",
    "    #comprehension list to generate all the csv files name needed\n",
    "    weather_csv_files = [f\"{year}_weather.csv\" for year in range(2009,2016)]\n",
    "    \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f900f7aa",
   "metadata": {},
   "source": [
    "### Process All Data\n",
    "\n",
    "_This is where you can actually execute all the required functions._\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with yellow_tripdata_2015-01.parquet\n",
      "done with yellow_tripdata_2015-02.parquet\n",
      "done with yellow_tripdata_2015-03.parquet\n",
      "done with yellow_tripdata_2015-04.parquet\n",
      "done with yellow_tripdata_2015-05.parquet\n",
      "done with yellow_tripdata_2015-06.parquet\n",
      "done with yellow_tripdata_2014-01.parquet\n",
      "done with yellow_tripdata_2014-02.parquet\n",
      "done with yellow_tripdata_2014-03.parquet\n",
      "done with yellow_tripdata_2014-04.parquet\n",
      "done with yellow_tripdata_2014-05.parquet\n",
      "done with yellow_tripdata_2014-06.parquet\n",
      "done with yellow_tripdata_2014-07.parquet\n",
      "done with yellow_tripdata_2014-08.parquet\n",
      "done with yellow_tripdata_2014-09.parquet\n",
      "done with yellow_tripdata_2014-10.parquet\n",
      "done with yellow_tripdata_2014-11.parquet\n",
      "done with yellow_tripdata_2014-12.parquet\n",
      "done with yellow_tripdata_2013-01.parquet\n",
      "done with yellow_tripdata_2013-02.parquet\n",
      "done with yellow_tripdata_2013-03.parquet\n",
      "done with yellow_tripdata_2013-04.parquet\n",
      "done with yellow_tripdata_2013-05.parquet\n",
      "done with yellow_tripdata_2013-06.parquet\n",
      "done with yellow_tripdata_2013-07.parquet\n",
      "done with yellow_tripdata_2013-08.parquet\n",
      "done with yellow_tripdata_2013-09.parquet\n",
      "done with yellow_tripdata_2013-10.parquet\n",
      "done with yellow_tripdata_2013-11.parquet\n",
      "done with yellow_tripdata_2013-12.parquet\n",
      "done with yellow_tripdata_2012-01.parquet\n",
      "done with yellow_tripdata_2012-02.parquet\n",
      "done with yellow_tripdata_2012-03.parquet\n",
      "done with yellow_tripdata_2012-04.parquet\n",
      "done with yellow_tripdata_2012-05.parquet\n",
      "done with yellow_tripdata_2012-06.parquet\n",
      "done with yellow_tripdata_2012-07.parquet\n",
      "done with yellow_tripdata_2012-08.parquet\n",
      "done with yellow_tripdata_2012-09.parquet\n",
      "done with yellow_tripdata_2012-10.parquet\n",
      "done with yellow_tripdata_2012-11.parquet\n",
      "done with yellow_tripdata_2012-12.parquet\n",
      "done with yellow_tripdata_2011-01.parquet\n",
      "done with yellow_tripdata_2011-02.parquet\n",
      "done with yellow_tripdata_2011-03.parquet\n",
      "done with yellow_tripdata_2011-04.parquet\n",
      "done with yellow_tripdata_2011-05.parquet\n",
      "done with yellow_tripdata_2011-06.parquet\n",
      "done with yellow_tripdata_2011-07.parquet\n",
      "done with yellow_tripdata_2011-08.parquet\n",
      "done with yellow_tripdata_2011-09.parquet\n",
      "done with yellow_tripdata_2011-10.parquet\n",
      "done with yellow_tripdata_2011-11.parquet\n",
      "done with yellow_tripdata_2011-12.parquet\n",
      "done with yellow_tripdata_2010-01.parquet\n",
      "done with yellow_tripdata_2010-02.parquet\n",
      "done with yellow_tripdata_2010-03.parquet\n",
      "done with yellow_tripdata_2010-04.parquet\n",
      "done with yellow_tripdata_2010-05.parquet\n",
      "done with yellow_tripdata_2010-06.parquet\n",
      "done with yellow_tripdata_2010-07.parquet\n",
      "done with yellow_tripdata_2010-08.parquet\n",
      "done with yellow_tripdata_2010-09.parquet\n",
      "done with yellow_tripdata_2010-10.parquet\n",
      "done with yellow_tripdata_2010-11.parquet\n",
      "done with yellow_tripdata_2010-12.parquet\n",
      "done with yellow_tripdata_2009-01.parquet\n",
      "done with yellow_tripdata_2009-02.parquet\n",
      "done with yellow_tripdata_2009-03.parquet\n",
      "done with yellow_tripdata_2009-04.parquet\n",
      "done with yellow_tripdata_2009-05.parquet\n",
      "done with yellow_tripdata_2009-06.parquet\n",
      "done with yellow_tripdata_2009-07.parquet\n",
      "done with yellow_tripdata_2009-08.parquet\n",
      "done with yellow_tripdata_2009-09.parquet\n",
      "done with yellow_tripdata_2009-10.parquet\n",
      "done with yellow_tripdata_2009-11.parquet\n",
      "done with yellow_tripdata_2009-12.parquet\n",
      "CPU times: user 24min 12s, sys: 20min 33s, total: 44min 46s\n",
      "Wall time: 39min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "taxi_data = get_and_clean_taxi_data()\n",
    "uber_data = get_uber_data()\n",
    "hourly_data, daily_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc767d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.to_csv('taxi_data.csv')\n",
    "uber_data.to_csv('uber_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data\n",
    "\n",
    "We are going to store all the data cleaned before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create a DataBase first\n",
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "\n",
    "#let's create the queries that will create all the tables needed\n",
    "\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    HourlyWindSpeed FLOAT32,\n",
    "    HourlyPrecipitation FLOAT32\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    DATE DATE,\n",
    "    DailyWindSpeed FLOAT32,\n",
    "    DailyPrecipitation FLOAT32\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    tip_amount FLOAT32,\n",
    "    pickup_latitude FLOAT32,\n",
    "    pickup_longitude FLOAT32,\n",
    "    dropoff_latitude FLOAT32,\n",
    "    dropoff_longitude FLOAT32,\n",
    "    distance FLOAT32\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    pickup_datetime DATE,\n",
    "    pickup_latitude FLOAT32,\n",
    "    pickup_longitude FLOAT32,\n",
    "    dropoff_latitude FLOAT32,\n",
    "    dropoff_longitude FLOAT32,\n",
    "    distance FLOAT32\n",
    ");\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    #open the file\n",
    "    fd = open(DATABASE_SCHEMA_FILE, 'r')\n",
    "    #get the content of the file\n",
    "    sqlFile = fd.read()\n",
    "    #close the file\n",
    "    fd.close()\n",
    "    #get the queries, which are separated by a ;\n",
    "    sql_queries = sqlFile.split(';')\n",
    "    \n",
    "    #execute each query, one at a time\n",
    "    for query in sql_queries:\n",
    "        connection.execute(query)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database\n",
    "\n",
    "We are going to fill the tables just created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    ''' converts all the pandas dataframe into corresponding sql tables\n",
    "    \n",
    "    Keyword arguments:\n",
    "    table_to_df_dict: a dictionary with name of the sql table as a key and dataframe as a value\n",
    "    '''\n",
    "    #we convert one by one, all the dataframes that we have in our dictionary\n",
    "    for table, dataframe in table_to_df_dict.items():\n",
    "        dataframe.to_sql(table, engine,if_exists='append', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1340c0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2069353</th>\n",
       "      <td>2015-01-06 17:29:48</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.748497</td>\n",
       "      <td>-73.99244</td>\n",
       "      <td>40.758030</td>\n",
       "      <td>-73.97770</td>\n",
       "      <td>1.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2955397</th>\n",
       "      <td>2015-01-08 19:33:23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.766950</td>\n",
       "      <td>-73.95963</td>\n",
       "      <td>40.765484</td>\n",
       "      <td>-73.95474</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508517</th>\n",
       "      <td>2015-01-05 07:00:59</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.748497</td>\n",
       "      <td>-73.99244</td>\n",
       "      <td>40.756687</td>\n",
       "      <td>-73.97236</td>\n",
       "      <td>1.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3492739</th>\n",
       "      <td>2015-01-09 22:01:56</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.717773</td>\n",
       "      <td>-74.00788</td>\n",
       "      <td>40.745380</td>\n",
       "      <td>-73.94889</td>\n",
       "      <td>5.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2759959</th>\n",
       "      <td>2015-01-08 10:10:21</td>\n",
       "      <td>1.5</td>\n",
       "      <td>40.773632</td>\n",
       "      <td>-73.98153</td>\n",
       "      <td>40.756687</td>\n",
       "      <td>-73.97236</td>\n",
       "      <td>2.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13043900</th>\n",
       "      <td>2009-12-26 08:27:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.746567</td>\n",
       "      <td>-73.99752</td>\n",
       "      <td>40.754227</td>\n",
       "      <td>-73.98176</td>\n",
       "      <td>1.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11516308</th>\n",
       "      <td>2009-12-22 07:32:26</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.770145</td>\n",
       "      <td>-73.95751</td>\n",
       "      <td>40.756283</td>\n",
       "      <td>-73.98517</td>\n",
       "      <td>2.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889599</th>\n",
       "      <td>2009-12-26 16:02:06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.774830</td>\n",
       "      <td>-73.98062</td>\n",
       "      <td>40.788177</td>\n",
       "      <td>-73.96725</td>\n",
       "      <td>1.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541093</th>\n",
       "      <td>2009-12-07 22:46:40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.759926</td>\n",
       "      <td>-73.97318</td>\n",
       "      <td>40.774296</td>\n",
       "      <td>-73.96361</td>\n",
       "      <td>1.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14517959</th>\n",
       "      <td>2009-12-23 07:46:23</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.685300</td>\n",
       "      <td>-73.99675</td>\n",
       "      <td>40.713448</td>\n",
       "      <td>-74.01114</td>\n",
       "      <td>3.36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185406 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              pickup_datetime  tip_amount  pickup_latitude  pickup_longitude  \\\n",
       "2069353   2015-01-06 17:29:48         0.0        40.748497         -73.99244   \n",
       "2955397   2015-01-08 19:33:23         0.0        40.766950         -73.95963   \n",
       "1508517   2015-01-05 07:00:59         1.0        40.748497         -73.99244   \n",
       "3492739   2015-01-09 22:01:56         0.0        40.717773         -74.00788   \n",
       "2759959   2015-01-08 10:10:21         1.5        40.773632         -73.98153   \n",
       "...                       ...         ...              ...               ...   \n",
       "13043900  2009-12-26 08:27:00         0.0        40.746567         -73.99752   \n",
       "11516308  2009-12-22 07:32:26         0.0        40.770145         -73.95751   \n",
       "889599    2009-12-26 16:02:06         0.0        40.774830         -73.98062   \n",
       "1541093   2009-12-07 22:46:40         0.0        40.759926         -73.97318   \n",
       "14517959  2009-12-23 07:46:23         0.0        40.685300         -73.99675   \n",
       "\n",
       "          dropoff_latitude  dropoff_longitude  distance  \n",
       "2069353          40.758030          -73.97770      1.63  \n",
       "2955397          40.765484          -73.95474      0.44  \n",
       "1508517          40.756687          -73.97236      1.92  \n",
       "3492739          40.745380          -73.94889      5.84  \n",
       "2759959          40.756687          -73.97236      2.04  \n",
       "...                    ...                ...       ...  \n",
       "13043900         40.754227          -73.98176      1.58  \n",
       "11516308         40.756283          -73.98517      2.79  \n",
       "889599           40.788177          -73.96725      1.86  \n",
       "1541093          40.774296          -73.96361      1.79  \n",
       "14517959         40.713448          -74.01114      3.36  \n",
       "\n",
       "[185406 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#++++++++++++++++++++++++FOR FINAL VERSION, REMOVE THIS +++++++++++++++++++++++++++\"\n",
    "taxi_data=pd.read_csv(\"taxi_data(1).csv\")\n",
    "uber_data=pd.read_csv(\"uber_data(1).csv\")\n",
    "hourly_data=pd.read_csv(\"hourly_data.csv\")\n",
    "daily_data=pd.read_csv(\"daily_data(1).csv\")\n",
    "\n",
    "taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dictionary with name of the sql table as a key and dataframe as a value\n",
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_data,\n",
    "    \"daily_weather\": daily_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill all the tables created before\n",
    "write_dataframes_to_table(map_table_name_to_dataframe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da1e35cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '2009-08-24 21:45:00+00:00', 40.74077, -74.00504, 40.772648, -73.96256, 5.04)\n",
      "(2, '2009-06-26 08:22:21+00:00', 40.790844, -73.97613, 40.80335, -73.96532, 1.66)\n",
      "(3, '2014-08-28 17:47:00+00:00', 40.744083, -73.925026, 40.761246, -73.97308, 4.48)\n",
      "(4, '2014-10-12 07:04:00+00:00', 40.693966, -73.96145, 40.774296, -73.87119, 11.73)\n",
      "(5, '2012-02-17 09:32:00+00:00', 40.745766, -73.97519, 40.743538, -74.00272, 2.33)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-08-24 21:45:00+00:00</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-74.005040</td>\n",
       "      <td>40.772648</td>\n",
       "      <td>-73.962560</td>\n",
       "      <td>5.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-06-26 08:22:21+00:00</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.976130</td>\n",
       "      <td>40.803350</td>\n",
       "      <td>-73.965320</td>\n",
       "      <td>1.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014-08-28 17:47:00+00:00</td>\n",
       "      <td>40.744083</td>\n",
       "      <td>-73.925026</td>\n",
       "      <td>40.761246</td>\n",
       "      <td>-73.973080</td>\n",
       "      <td>4.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2014-10-12 07:04:00+00:00</td>\n",
       "      <td>40.693966</td>\n",
       "      <td>-73.961450</td>\n",
       "      <td>40.774296</td>\n",
       "      <td>-73.871190</td>\n",
       "      <td>11.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2012-02-17 09:32:00+00:00</td>\n",
       "      <td>40.745766</td>\n",
       "      <td>-73.975190</td>\n",
       "      <td>40.743538</td>\n",
       "      <td>-74.002720</td>\n",
       "      <td>2.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199994</th>\n",
       "      <td>2014-01-31 14:42:00+00:00</td>\n",
       "      <td>40.760770</td>\n",
       "      <td>-73.983070</td>\n",
       "      <td>40.754177</td>\n",
       "      <td>-73.972970</td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199996</th>\n",
       "      <td>2014-03-14 01:09:00+00:00</td>\n",
       "      <td>40.736835</td>\n",
       "      <td>-73.984720</td>\n",
       "      <td>40.739620</td>\n",
       "      <td>-74.006676</td>\n",
       "      <td>1.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199997</th>\n",
       "      <td>2009-06-29 00:42:00+00:00</td>\n",
       "      <td>40.756490</td>\n",
       "      <td>-73.986015</td>\n",
       "      <td>40.692590</td>\n",
       "      <td>-73.858955</td>\n",
       "      <td>12.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199998</th>\n",
       "      <td>2015-05-20 14:56:25+00:00</td>\n",
       "      <td>40.725452</td>\n",
       "      <td>-73.997120</td>\n",
       "      <td>40.695415</td>\n",
       "      <td>-73.983215</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199999</th>\n",
       "      <td>2010-05-15 04:08:00+00:00</td>\n",
       "      <td>40.720078</td>\n",
       "      <td>-73.984400</td>\n",
       "      <td>40.768790</td>\n",
       "      <td>-73.985510</td>\n",
       "      <td>5.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>184831 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  pickup_datetime  pickup_latitude  pickup_longitude  \\\n",
       "2       2009-08-24 21:45:00+00:00        40.740770        -74.005040   \n",
       "3       2009-06-26 08:22:21+00:00        40.790844        -73.976130   \n",
       "4       2014-08-28 17:47:00+00:00        40.744083        -73.925026   \n",
       "6       2014-10-12 07:04:00+00:00        40.693966        -73.961450   \n",
       "8       2012-02-17 09:32:00+00:00        40.745766        -73.975190   \n",
       "...                           ...              ...               ...   \n",
       "199994  2014-01-31 14:42:00+00:00        40.760770        -73.983070   \n",
       "199996  2014-03-14 01:09:00+00:00        40.736835        -73.984720   \n",
       "199997  2009-06-29 00:42:00+00:00        40.756490        -73.986015   \n",
       "199998  2015-05-20 14:56:25+00:00        40.725452        -73.997120   \n",
       "199999  2010-05-15 04:08:00+00:00        40.720078        -73.984400   \n",
       "\n",
       "        dropoff_latitude  dropoff_longitude  distance  \n",
       "2              40.772648         -73.962560      5.04  \n",
       "3              40.803350         -73.965320      1.66  \n",
       "4              40.761246         -73.973080      4.48  \n",
       "6              40.774296         -73.871190     11.73  \n",
       "8              40.743538         -74.002720      2.33  \n",
       "...                  ...                ...       ...  \n",
       "199994         40.754177         -73.972970      1.12  \n",
       "199996         40.739620         -74.006676      1.88  \n",
       "199997         40.692590         -73.858955     12.85  \n",
       "199998         40.695415         -73.983215      3.54  \n",
       "199999         40.768790         -73.985510      5.42  \n",
       "\n",
       "[184831 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TEST CELL\n",
    "\n",
    "result=engine.execute(\"SELECT * FROM uber_trips LIMIT 5\").fetchall()\n",
    "\n",
    "for row in result:\n",
    "    print(row)\n",
    "    \n",
    "uber_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4753fcd",
   "metadata": {},
   "source": [
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] For 01-2009 through 06-2015, what hour of the day was the most popular to take a yellow taxi? The result should have 24 bins.\n",
    "* [ ] For the same time frame, what day of the week was the most popular to take an uber? The result should have 7 bins.\n",
    "* [ ] What is the 95% percentile of distance traveled for all hired trips during July 2013?\n",
    "* [ ] What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day?\n",
    "* [ ] Which 10 days in 2014 were the windiest, and how many hired trips were made on those days?\n",
    "* [ ] During Hurricane Sandy in NYC (Oct 29-30, 2012) and the week leading up to it, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d8cc9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>DailyWindSpeed</th>\n",
       "      <th>DailyPrecipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01</td>\n",
       "      <td>11.04</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-02</td>\n",
       "      <td>6.81</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-03</td>\n",
       "      <td>9.88</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-04</td>\n",
       "      <td>7.37</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-05</td>\n",
       "      <td>6.93</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>2015-12-27</td>\n",
       "      <td>4.91</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>2015-12-28</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>2015-12-29</td>\n",
       "      <td>7.79</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>2015-12-30</td>\n",
       "      <td>4.18</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>4.74</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2551 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           DATE  DailyWindSpeed  DailyPrecipitation\n",
       "0    2009-01-01           11.04                0.00\n",
       "1    2009-01-02            6.81                0.00\n",
       "2    2009-01-03            9.88                0.00\n",
       "3    2009-01-04            7.37                0.00\n",
       "4    2009-01-05            6.93                0.00\n",
       "..          ...             ...                 ...\n",
       "360  2015-12-27            4.91                0.00\n",
       "361  2015-12-28            8.21                0.00\n",
       "362  2015-12-29            7.79                0.02\n",
       "363  2015-12-30            4.18                0.01\n",
       "364  2015-12-31            4.74                0.00\n",
       "\n",
       "[2551 rows x 3 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "daily_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_query_to_file(query, outfile):\n",
    "    \n",
    "    '''It writes the given query on to the file\n",
    "\n",
    "    Keyword arguments:\n",
    "    query -- Query to write on the file\n",
    "    outfile -- This is the file where the query will be written \n",
    "    '''\n",
    "    with open(QUERY_DIRECTORY + outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each query_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1 = \"\"\"\n",
    "SELECT COUNT(Id) as num,strftime ('%H', pickup_datetime) \n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime between '2009-01-01' and '2015-06-31'\n",
    "GROUP BY strftime ('%H',pickup_datetime)\n",
    "ORDER BY num DESC \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(11616, '19'),\n",
       " (11139, '18'),\n",
       " (11012, '20'),\n",
       " (10683, '21'),\n",
       " (10373, '22'),\n",
       " (9270, '14'),\n",
       " (9230, '23'),\n",
       " (9068, '17'),\n",
       " (9057, '12'),\n",
       " (9015, '13'),\n",
       " (8960, '15'),\n",
       " (8787, '09'),\n",
       " (8555, '11'),\n",
       " (8337, '08'),\n",
       " (8332, '10'),\n",
       " (7602, '16'),\n",
       " (7219, '00'),\n",
       " (6724, '07'),\n",
       " (5668, '01'),\n",
       " (4044, '02'),\n",
       " (3812, '06'),\n",
       " (2938, '03'),\n",
       " (2132, '04'),\n",
       " (1833, '05')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2ef04df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, \"most_popular_hour_to_take_yellow_taxi.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e30f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2 = \"\"\"\n",
    "SELECT COUNT(Id) as num,strftime ('%w', pickup_datetime) \n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime between '2009-01-01' and '2015-06-31'\n",
    "GROUP BY strftime ('%w',pickup_datetime)\n",
    "ORDER BY num DESC \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dbe9c45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(28518, '5'),\n",
       " (28028, '6'),\n",
       " (27786, '4'),\n",
       " (26823, '3'),\n",
       " (25953, '2'),\n",
       " (24418, '0'),\n",
       " (23305, '1')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea687102",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, \"most_popular_day_to_take_uber.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e22e3065",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3= \"\"\"\n",
    "WITH all_hired_trips AS (\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime between '2013-07-01' and '2013-07-31'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime between '2013-07-01' and '2013-08-01'\n",
    ")\n",
    "SELECT distance\n",
    "FROM all_hired_trips\n",
    "ORDER BY distance \n",
    "LIMIT 1\n",
    "OFFSET (SELECT COUNT(*) FROM all_hired_trips) * 95 / 100 - 1\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30371441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10.23,)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da702612",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, \"95_percentile_all_hired_trips.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "81c86796",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4= \"\"\"\n",
    "WITH all_hired_trips AS (\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime between '2009-01-01' and '2010-01-01'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime between '2009-01-01' and '2010-01-01'\n",
    ")\n",
    "SELECT date(pickup_datetime) as day, COUNT(*) as num_rides, AVG(distance) as avg_distance\n",
    "FROM all_hired_trips\n",
    "GROUP BY date(pickup_datetime)\n",
    "ORDER BY num_rides DESC\n",
    "LIMIT 10\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5960c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2009-12-11', 211, 3.2616113744075843),\n",
       " ('2009-07-09', 202, 3.1860396039603955),\n",
       " ('2009-07-23', 201, 3.527014925373137),\n",
       " ('2009-02-21', 201, 3.3720895522388075),\n",
       " ('2009-10-23', 200, 3.0523999999999996),\n",
       " ('2009-02-06', 200, 3.139699999999999),\n",
       " ('2009-09-12', 199, 3.2888944723618123),\n",
       " ('2009-01-31', 197, 3.012893401015228),\n",
       " ('2009-02-20', 194, 3.0074226804123727),\n",
       " ('2009-08-14', 193, 3.6655440414507763)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3aa917ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, \"2009_trips_data.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1d2ada7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5= \"\"\"\n",
    "WITH all_hired_trips AS (\n",
    "    SELECT date(pickup_datetime) as pickup_date, distance\n",
    "    FROM taxi_trips\n",
    "    UNION ALL\n",
    "    SELECT date(pickup_datetime) as pickup_date, distance\n",
    "    FROM uber_trips\n",
    "),\n",
    "windiest_days AS (\n",
    "    SELECT DATE, DailyWindSpeed\n",
    "    FROM daily_weather\n",
    "    WHERE DATE between '2014-01-01' and '2015-01-01'\n",
    "    ORDER BY DailyWindSpeed DESC\n",
    "    LIMIT 10\n",
    ")\n",
    "SELECT wd.DATE, COUNT(*)\n",
    "FROM windiest_days as wd, all_hired_trips as ht\n",
    "WHERE wd.DATE == ht.pickup_date\n",
    "GROUP BY wd.DATE\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a61cffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2014-01-02', 121),\n",
       " ('2014-01-07', 154),\n",
       " ('2014-02-13', 119),\n",
       " ('2014-03-13', 191),\n",
       " ('2014-03-26', 174),\n",
       " ('2014-03-29', 203),\n",
       " ('2014-11-02', 153),\n",
       " ('2014-12-07', 166),\n",
       " ('2014-12-08', 141),\n",
       " ('2014-12-09', 127)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38e278e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, \"2014_windiest_days_stats.sql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7c3a57f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:51:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:51:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:51:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 03:51:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 04:51:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11379</th>\n",
       "      <td>2015-12-31 18:51:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11380</th>\n",
       "      <td>2015-12-31 19:51:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11381</th>\n",
       "      <td>2015-12-31 20:51:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11383</th>\n",
       "      <td>2015-12-31 22:51:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11384</th>\n",
       "      <td>2015-12-31 23:51:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73713 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      DATE  HourlyWindSpeed  HourlyPrecipitation\n",
       "0      2009-01-01 00:51:00             18.0                  0.0\n",
       "1      2009-01-01 01:51:00             18.0                  0.0\n",
       "2      2009-01-01 02:51:00             18.0                  0.0\n",
       "3      2009-01-01 03:51:00              8.0                  0.0\n",
       "4      2009-01-01 04:51:00             11.0                  0.0\n",
       "...                    ...              ...                  ...\n",
       "11379  2015-12-31 18:51:00              3.0                  0.0\n",
       "11380  2015-12-31 19:51:00              6.0                  0.0\n",
       "11381  2015-12-31 20:51:00             10.0                  0.0\n",
       "11383  2015-12-31 22:51:00              7.0                  0.0\n",
       "11384  2015-12-31 23:51:00              5.0                  0.0\n",
       "\n",
       "[73713 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "49187a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY='''\n",
    "SELECT strftime ('%Y %m %d %H', DATE) as date_hour, AVG(HourlyWindSpeed) as hourly_wind_speed, AVG(HourlyPrecipitation) as hourly_precipitation \n",
    "FROM hourly_weather\n",
    "WHERE date(DATE) between '2010-01-01' and '2010-01-01'\n",
    "GROUP BY date_hour\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cbb37e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2010 01 01 00', 1.5, 0.01),\n",
       " ('2010 01 01 01', 0.0, 0.005),\n",
       " ('2010 01 01 02', 0.0, 0.0),\n",
       " ('2010 01 01 03', 0.0, 0.0),\n",
       " ('2010 01 01 04', 0.0, 0.0),\n",
       " ('2010 01 01 05', 3.0, 0.0),\n",
       " ('2010 01 01 06', 1.5, 0.0),\n",
       " ('2010 01 01 07', 3.0, 0.0),\n",
       " ('2010 01 01 08', 4.333333333333333, 0.0),\n",
       " ('2010 01 01 09', 2.5, 0.0),\n",
       " ('2010 01 01 10', 7.0, 0.0),\n",
       " ('2010 01 01 11', 5.0, 0.0),\n",
       " ('2010 01 01 12', 3.0, 0.0),\n",
       " ('2010 01 01 13', 0.0, 0.0),\n",
       " ('2010 01 01 14', 5.0, 0.0),\n",
       " ('2010 01 01 15', 0.0, 0.0),\n",
       " ('2010 01 01 16', 6.0, 0.0),\n",
       " ('2010 01 01 17', 5.0, 0.0),\n",
       " ('2010 01 01 18', 3.0, 0.0),\n",
       " ('2010 01 01 19', 6.0, 0.0),\n",
       " ('2010 01 01 20', 9.0, 0.0),\n",
       " ('2010 01 01 21', 0.0, 0.0),\n",
       " ('2010 01 01 22', 6.5, 0.0),\n",
       " ('2010 01 01 23', 9.0, 0.01)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.execute(QUERY).fetchall()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5b8d0ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>HourlyWindSpeed</th>\n",
       "      <th>HourlyPrecipitation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009-01-01 00:51:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2009-01-01 01:51:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009-01-01 02:51:00</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009-01-01 03:51:00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2009-01-01 04:51:00</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11379</th>\n",
       "      <td>2015-12-31 18:51:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11380</th>\n",
       "      <td>2015-12-31 19:51:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11381</th>\n",
       "      <td>2015-12-31 20:51:00</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11383</th>\n",
       "      <td>2015-12-31 22:51:00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11384</th>\n",
       "      <td>2015-12-31 23:51:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73713 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      DATE  HourlyWindSpeed  HourlyPrecipitation\n",
       "0      2009-01-01 00:51:00             18.0                  0.0\n",
       "1      2009-01-01 01:51:00             18.0                  0.0\n",
       "2      2009-01-01 02:51:00             18.0                  0.0\n",
       "3      2009-01-01 03:51:00              8.0                  0.0\n",
       "4      2009-01-01 04:51:00             11.0                  0.0\n",
       "...                    ...              ...                  ...\n",
       "11379  2015-12-31 18:51:00              3.0                  0.0\n",
       "11380  2015-12-31 19:51:00              6.0                  0.0\n",
       "11381  2015-12-31 20:51:00             10.0                  0.0\n",
       "11383  2015-12-31 22:51:00              7.0                  0.0\n",
       "11384  2015-12-31 23:51:00              5.0                  0.0\n",
       "\n",
       "[73713 rows x 3 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hourly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a1bb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6='''\n",
    "WITH hourly_data AS (\n",
    "SELECT strftime ('%Y %m %d %H', DATE) as date_hour, AVG(HourlyWindSpeed) as hourly_wind_speed, AVG(HourlyPrecipitation) as hourly_precipitation \n",
    "FROM hourly_weather\n",
    "GROUP BY date_hour ),\n",
    "\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bda36861",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_7='''\n",
    "WITH RECURSIVE date_hour(x) AS ( \n",
    "            SELECT '2015-01-01' \n",
    "                UNION ALL \n",
    "            SELECT DATE(x, '+1 HOUR') FROM date_hour WHERE x<'2015-01-02' \n",
    "        ) \n",
    "SELECT *\n",
    "FROM date_hour\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bbd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_7).fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435cde8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_7).fetchall()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade5608",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc3c982",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data\n",
    "\n",
    "_A checklist of requirements to keep you on track. Remove this whole cell before submitting the project. The order of these tasks aren't necessarily the order in which they need to be done. It's okay to do them in an order that makes sense to you._\n",
    "\n",
    "* [ ] Create an appropriate visualization for the first query/question in part 3\n",
    "* [ ] Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month). Include the 90% confidence interval around the mean in the visualization\n",
    "* [ ] Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.\n",
    "* [ ] Create a heatmap of all hired trips over a map of the area. Consider using KeplerGL or another library that helps generate geospatial visualizations.\n",
    "* [ ] Create a scatter plot that compares tip amount versus distance.\n",
    "* [ ] Create another scatter plot that compares tip amount versus precipitation amount.\n",
    "\n",
    "_Be sure these cells are executed so that the visualizations are rendered when the notebook is submitted._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization N\n",
    "\n",
    "_**TODO:** Write some prose that tells the reader what you're about to do here._\n",
    "\n",
    "_Repeat for each visualization._\n",
    "\n",
    "_The example below makes use of the `matplotlib` library. There are other libraries, including `pandas` built-in plotting library, kepler for geospatial data representation, `seaborn`, and others._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a more descriptive name for your function\n",
    "def plot_visual_n(dataframe):\n",
    "    figure, axes = plt.subplots(figsize=(20, 10))\n",
    "    \n",
    "    values = \"...\"  # use the dataframe to pull out values needed to plot\n",
    "    \n",
    "    # you may want to use matplotlib to plot your visualizations;\n",
    "    # there are also many other plot types (other \n",
    "    # than axes.plot) you can use\n",
    "    axes.plot(values, \"...\")\n",
    "    # there are other methods to use to label your axes, to style \n",
    "    # and set up axes labels, etc\n",
    "    axes.set_title(\"Some Descriptive Title\")\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_n():\n",
    "    # Query SQL database for the data needed.\n",
    "    # You can put the data queried into a pandas dataframe, if you wish\n",
    "    raise NotImplemented()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_n()\n",
    "plot_visual_n(some_dataframe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
